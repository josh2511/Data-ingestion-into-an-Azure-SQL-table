{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Load data into an Azure SQL non-partitioned table\n",
    "\n",
    "The sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). Make sure you have installed it before running this notebook. Maven Coordinates: `com.microsoft.azure:spark-mssql-connector:1.0.0`\n",
    "\n",
    "## Notes on terminology\n",
    "\n",
    "The term \"row-store\" is used to identify and index that is not using to store its data.\n",
    "\n",
    "## Samples\n",
    "\n",
    "In this notebook there are three samples\n",
    "\n",
    "- Load data into a table without indexes\n",
    "- Load data into a table with row-store indexes\n",
    "- Load data into a table with columns-store indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">scope: String = key-vault-secrets\n",
       "storageAccount: String = dmstore2\n",
       "storageKey: String = [REDACTED]\n",
       "server: String = [REDACTED].database.windows.net\n",
       "database: String = ApacheSpark\n",
       "user: String = [REDACTED]\n",
       "password: String = [REDACTED]\n",
       "table: String = dbo.LINEITEM_LOADTEST\n",
       "url: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=ApacheSpark;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val scope = \"key-vault-secrets\"\n",
    "\n",
    "val storageAccount = \"dmstore2\";\n",
    "val storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n",
    "\n",
    "val server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\n",
    "val database = \"ApacheSpark\";\n",
    "val user = dbutils.secrets.get(scope, \"dbuser001\");\n",
    "val password = dbutils.secrets.get(scope, \"dbpwd001\");\n",
    "val table = \"dbo.LINEITEM_LOADTEST\"\n",
    "\n",
    "val url = s\"jdbc:sqlserver://$server;databaseName=$database;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Spark to access Azure Blob Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">li: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val li = spark.read.parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded data is split in 20 dataframe partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res2: Int = 20\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show schema of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- L_ORDERKEY: integer (nullable = true)\n",
       "-- L_PARTKEY: integer (nullable = true)\n",
       "-- L_SUPPKEY: integer (nullable = true)\n",
       "-- L_LINENUMBER: integer (nullable = true)\n",
       "-- L_QUANTITY: decimal(15,2) (nullable = true)\n",
       "-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n",
       "-- L_DISCOUNT: decimal(15,2) (nullable = true)\n",
       "-- L_TAX: decimal(15,2) (nullable = true)\n",
       "-- L_RETURNFLAG: string (nullable = true)\n",
       "-- L_LINESTATUS: string (nullable = true)\n",
       "-- L_SHIPDATE: date (nullable = true)\n",
       "-- L_COMMITDATE: date (nullable = true)\n",
       "-- L_RECEIPTDATE: date (nullable = true)\n",
       "-- L_SHIPINSTRUCT: string (nullable = true)\n",
       "-- L_SHIPMODE: string (nullable = true)\n",
       "-- L_COMMENT: string (nullable = true)\n",
       "-- L_PARTITION_KEY: integer (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make sure you create on your Azure SQL the following LINEITEM table:\n",
    "```sql\n",
    "create table [dbo].[LINEITEM_LOADTEST]\n",
    "(\n",
    "\t[L_ORDERKEY] [int] not null,\n",
    "\t[L_PARTKEY] [int] not null,\n",
    "\t[L_SUPPKEY] [int] not null,\n",
    "\t[L_LINENUMBER] [int] not null,\n",
    "\t[L_QUANTITY] [decimal](15, 2) not null,\n",
    "\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n",
    "\t[L_DISCOUNT] [decimal](15, 2) not null,\n",
    "\t[L_TAX] [decimal](15, 2) not null,\n",
    "\t[L_RETURNFLAG] [char](1) not null,\n",
    "\t[L_LINESTATUS] [char](1) not null,\n",
    "\t[L_SHIPDATE] [date] not null,\n",
    "\t[L_COMMITDATE] [date] not null,\n",
    "\t[L_RECEIPTDATE] [date] not null,\n",
    "\t[L_SHIPINSTRUCT] [char](25) not null,\n",
    "\t[L_SHIPMODE] [char](10) not null,\n",
    "\t[L_COMMENT] [varchar](44) not null,\n",
    "\t[L_PARTITION_KEY] [int] not null\n",
    ") \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a table with no indexes\n",
    "\n",
    "In Azure SQL terminology an Heap is a table with no clustered index. In this sample we'll load data into a table that as no index (clustered or non-clustered) as is not partitioned. This is the simplest scenario possibile and allows parallel load of data.\n",
    "\n",
    "### Note:\n",
    "Parallel load *cannot* happen if you have row-store indexes on the table. If you want to bulk load data in parallel into a table that has row-store indexes, you must use partitioning. If you are planning to add indexes to your table, and data to be loaded in the table is in the terabyte range, you want to use partitioing and have indexes created before bulk loading data into Azure SQL, as otherwise creating index once the table is already loaded will use a significat amout of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable parallel load the option `tableLock` must be set to `true`. This will prevent any other access to the table, other then the one done for performing the bulk load operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.write\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n",
    "  .mode(\"overwrite\")   \n",
    "  .option(\"truncate\", \"true\") \n",
    "  .option(\"url\", url) \n",
    "  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n",
    "  .option(\"user\", user) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n",
    "  .option(\"tableLock\", \"true\") \n",
    "  .option(\"batchsize\", \"100000\")\n",
    "  .option(\"schemaCheckEnabled\", \"false\") // needed to avoid clash of NULLable columns vs NON-NULLable colums\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a table with row-store indexes\n",
    "\n",
    "If table is not partitioned, there are no options to bulk load data in parallel into the desired table. The only way to avoid locking and deadlocks is to load everything by serializing the bulk load operations. As you can expect, performance won't be the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following index on the table\n",
    "```sql\n",
    "create clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]);\n",
    "\n",
    "create unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER]);\n",
    "\n",
    "create nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY]); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data by coalescing all dataframe partitions into just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val url = s\"jdbc:sqlserver://$server;databaseName=$database;\"\n",
    "\n",
    "li.coalesce(1)\n",
    "  .write\n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n",
    "  .mode(\"overwrite\")   \n",
    "  .option(\"truncate\", \"true\") \n",
    "  .option(\"url\", url) \n",
    "  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n",
    "  .option(\"user\", user) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n",
    "  .option(\"tableLock\", \"false\") \n",
    "  .option(\"batchsize\", \"100000\")\n",
    "  .option(\"schemaCheckEnabled\", \"false\")\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a table with (only) column-store indexes\n",
    "\n",
    "If a table has only column-store indexes, data load can happen in parallel, as there is no sorting needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty table if needed, to speed up index deletion\n",
    "\n",
    "```sql\n",
    "truncate table dbo.[LINEITEM_LOADTEST];\n",
    "```\n",
    "\n",
    "Drop the previously create indexes if needed:\n",
    "```sql\n",
    "drop index IXC on dbo.[LINEITEM_LOADTEST];\n",
    "drop index IX1 on dbo.[LINEITEM_LOADTEST];\n",
    "drop index IX2 on dbo.[LINEITEM_LOADTEST];\n",
    "```\n",
    "\n",
    "And the create a clustered columnstore index:\n",
    "\n",
    "```sql\n",
    "create clustered columnstore index IXCCS on dbo.[LINEITEM_LOADTEST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading 1048576 rows at time, to land directly into a compressed segment. `tableLock` options must be set to `false` to avoid table lock that will prevent parallel load. Data with be loaded in parallel, using as many as Apache Spark workers are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.write \n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n",
    "  .mode(\"overwrite\")   \n",
    "  .option(\"truncate\", \"true\") \n",
    "  .option(\"url\", url) \n",
    "  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n",
    "  .option(\"user\", user) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n",
    "  .option(\"tableLock\", \"false\") \n",
    "  .option(\"batchsize\", \"1048576\")   \n",
    "  .option(\"schemaCheckEnabled\", \"false\")\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "name": "01-load-into-single-table",
  "notebookId": 1331848450253174
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
