{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into an Azure SQL partitioned table\n",
    "\n",
    "Azure SQL supports [table and index partitioning](https://docs.microsoft.com/en-us/sql/relational-databases/partitions/partitioned-tables-and-indexes). If a table is partitioned, data can be loaded in parallel without the need to put a lock on the entire table. In order to allow parallel partitions to be loaded, the source RDD/DataFrame/Dataset and the target Azure SQL table *MUST* have compatible partitions, which means that one RDD partition ends up exactly in one or more than one Azure SQL partitions, and those are not used by other RDD partitions.\n",
    "\n",
    "When table is partitioned, data *can* be bulk loaded in parallel also if there are indexes on the table. Especially on very large databases _this is the recommended approach_. The bulk load process will be a bit slower, but you'll not need to create indexes after having loaded the data. Creation of indexes on huge, already loaded, tables is a very expensive operation that you would like to avoid if possibile.\n",
    "\n",
    "\n",
    "\n",
    "## Dataframe and Azure SQL partitions\n",
    "\n",
    "Both Azure SQL and Azure Databricks (more specifically, Spark, and even more specifically a Spark Dataframe) are able to use take advantage of partitioning to more easily deal with large amounts of data. Partitions allow to work on subset of data, and usually you can do such work in parallel, spreading the workload on several CPU and/or nodes.\n",
    "\n",
    "## Notes on terminology\n",
    "\n",
    "The term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n",
    "\n",
    "## Samples\n",
    "\n",
    "In this notebook there are two samples\n",
    "\n",
    "- Load data into a partitioned table with row-store indexes\n",
    "- Load data into a partitioned table with columns-store indexes\n",
    "\n",
    "## Supported Azure Databricks Versions\n",
    "\n",
    "Databricks supported versions: Spark 2.4.5 and Scala 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">scope: String = key-vault-secrets\n",
       "storageAccount: String = dmstore2\n",
       "storageKey: String = [REDACTED]\n",
       "server: String = [REDACTED].database.windows.net\n",
       "database: String = ApacheSpark\n",
       "user: String = [REDACTED]\n",
       "password: String = [REDACTED]\n",
       "url: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=ApacheSpark;\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val scope = \"key-vault-secrets\"\n",
    "\n",
    "val storageAccount = \"dmstore2\";\n",
    "val storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n",
    "\n",
    "val server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\n",
    "val database = \"ApacheSpark\";\n",
    "val user = dbutils.secrets.get(scope, \"dbuser001\");\n",
    "val password = dbutils.secrets.get(scope, \"dbpwd001\");\n",
    "\n",
    "val url = s\"jdbc:sqlserver://$server;databaseName=$database;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Spark to access Azure Blob Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">li: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val li = spark.read.parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded data is split in 20 dataframe partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res2: Int = 20\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show schema of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">root\n",
       "-- L_ORDERKEY: integer (nullable = true)\n",
       "-- L_PARTKEY: integer (nullable = true)\n",
       "-- L_SUPPKEY: integer (nullable = true)\n",
       "-- L_LINENUMBER: integer (nullable = true)\n",
       "-- L_QUANTITY: decimal(15,2) (nullable = true)\n",
       "-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n",
       "-- L_DISCOUNT: decimal(15,2) (nullable = true)\n",
       "-- L_TAX: decimal(15,2) (nullable = true)\n",
       "-- L_RETURNFLAG: string (nullable = true)\n",
       "-- L_LINESTATUS: string (nullable = true)\n",
       "-- L_SHIPDATE: date (nullable = true)\n",
       "-- L_COMMITDATE: date (nullable = true)\n",
       "-- L_RECEIPTDATE: date (nullable = true)\n",
       "-- L_SHIPINSTRUCT: string (nullable = true)\n",
       "-- L_SHIPMODE: string (nullable = true)\n",
       "-- L_COMMENT: string (nullable = true)\n",
       "-- L_PARTITION_KEY: integer (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you create on your Azure SQL the following `LINEITEM` table, partitioned by `L_PARTITION_KEY`:\n",
    "\n",
    "```sql\n",
    "create partition function pf_LINEITEM(int)\n",
    "as range left for values \n",
    "(\n",
    "\t199201,199202,199203,199204,199205,199206,199207,199208,199209,199210,199211,199212,\n",
    "\t199301,199302,199303,199304,199305,199306,199307,199308,199309,199310,199311,199312,\n",
    "\t199401,199402,199403,199404,199405,199406,199407,199408,199409,199410,199411,199412,\n",
    "\t199501,199502,199503,199504,199505,199506,199507,199508,199509,199510,199511,199512,\n",
    "\t199601,199602,199603,199604,199605,199606,199607,199608,199609,199610,199611,199612,\n",
    "\t199701,199702,199703,199704,199705,199706,199707,199708,199709,199710,199711,199712,\n",
    "\t199801,199802,199803,199804,199805,199806,199807,199808,199809,199810\n",
    ");\n",
    "\n",
    "create partition scheme ps_LINEITEM\n",
    "as partition pf_LINEITEM\n",
    "all to ([Primary])\n",
    ";\n",
    "\n",
    "drop table if exists [dbo].[LINEITEM_LOADTEST];\n",
    "create table [dbo].[LINEITEM_LOADTEST]\n",
    "(\n",
    "\t[L_ORDERKEY] [int] not null,\n",
    "\t[L_PARTKEY] [int] not null,\n",
    "\t[L_SUPPKEY] [int] not null,\n",
    "\t[L_LINENUMBER] [int] not null,\n",
    "\t[L_QUANTITY] [decimal](15, 2) not null,\n",
    "\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n",
    "\t[L_DISCOUNT] [decimal](15, 2) not null,\n",
    "\t[L_TAX] [decimal](15, 2) not null,\n",
    "\t[L_RETURNFLAG] [char](1) not null,\n",
    "\t[L_LINESTATUS] [char](1) not null,\n",
    "\t[L_SHIPDATE] [date] not null,\n",
    "\t[L_COMMITDATE] [date] not null,\n",
    "\t[L_RECEIPTDATE] [date] not null,\n",
    "\t[L_SHIPINSTRUCT] [char](25) not null,\n",
    "\t[L_SHIPMODE] [char](10) not null,\n",
    "\t[L_COMMENT] [varchar](44) not null,\n",
    "\t[L_PARTITION_KEY] [int] not null\n",
    ") on ps_LINEITEM([L_PARTITION_KEY])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that Azure SQL table is partitioned by running the following T-SQL command:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    schema_name(t.schema_id) as [schema_name],\n",
    "    t.[name] as table_name,\n",
    "    i.[name] as index_name,\n",
    "    ps.[partition_id],\n",
    "    ps.partition_number,\n",
    "    p.data_compression_desc,\n",
    "    i.[type_desc],    \n",
    "    ps.row_count,\n",
    "    (ps.used_page_count * 8.) / 1024. / 1024. as size_in_gb\n",
    "from\n",
    "    sys.dm_db_partition_stats as ps \n",
    "inner join  \n",
    "    sys.partitions as p on ps.partition_id = p.partition_id\n",
    "inner join\n",
    "    sys.tables as t on t.object_id = ps.object_id\n",
    "inner join\n",
    "    sys.indexes as i on ps.object_id = i.object_id and ps.index_id = i.index_id\n",
    "where\n",
    "    t.[name] = 'LINEITEM_LOADTEST' and t.[schema_id] = schema_id('dbo')\n",
    "order by\n",
    "    [schema_name], table_name, index_name, partition_number\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a partitioned table with row-store indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the target table create the Clustered Index and a couple of Non-Clustered Index. In order to allow parallel partitioned load, also indexes must use the same partitioning function used by the table\n",
    "\n",
    "```sql\n",
    "create clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]) \n",
    "on ps_LINEITEM([L_PARTITION_KEY]);\n",
    "\n",
    "create unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY]) \n",
    "on ps_LINEITEM([L_PARTITION_KEY]);\n",
    "\n",
    "create nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY], [L_PARTITION_KEY]) \n",
    "on ps_LINEITEM([L_PARTITION_KEY]);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As DataFrame and Azure SQL Table are both partitioned by `L_PARTITION_KEY`, there isn't much left to do and the connector will take care of everything for us. `tableLock` must be set to `false` to avoid table lock that will prevent parallel partitioned load. Thanks to partitions, acquired locks will not interfere with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li.write \n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n",
    "  .mode(\"overwrite\")   \n",
    "  .option(\"truncate\", \"true\") \n",
    "  .option(\"url\", url) \n",
    "  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n",
    "  .option(\"user\", user) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n",
    "  .option(\"tableLock\", \"false\") \n",
    "  .option(\"batchsize\", \"100000\") \n",
    "  .option(\"schemaCheckEnabled\", \"false\") // needed to avoid clash of NULLable columns vs NON-NULLable colums\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a partitioned table with column-store index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty the test table if needed, to speed up index deletion\n",
    "\n",
    "```sql\n",
    "truncate table dbo.[LINEITEM_LOADTEST];\n",
    "```\n",
    "\n",
    "Drop the previously create indexes if needed:\n",
    "```sql\n",
    "drop index IXC on dbo.[LINEITEM_LOADTEST];\n",
    "drop index IX1 on dbo.[LINEITEM_LOADTEST];\n",
    "drop index IX2 on dbo.[LINEITEM_LOADTEST];\n",
    "```\n",
    "\n",
    "And then create a clustered columnstore index:\n",
    "\n",
    "```sql\n",
    "create clustered columnstore index IXCCS on dbo.[LINEITEM_LOADTEST]\n",
    "on ps_LINEITEM([L_PARTITION_KEY]);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading 1048576 rows at time, to land directly into a compressed segment. Locking the table must be set to `false` to avoid locking. Data with be loaded in parallel, using as many as Apache Spark workers are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.write \n",
    "  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n",
    "  .mode(\"overwrite\")   \n",
    "  .option(\"truncate\", \"true\") \n",
    "  .option(\"url\", url) \n",
    "  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n",
    "  .option(\"user\", user) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n",
    "  .option(\"tableLock\", \"false\") \n",
    "  .option(\"batchsize\", \"1048576\") \n",
    "  .option(\"schemaCheckEnabled\", \"false\")\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "name": "02-load-into-partitioned-table",
  "notebookId": 1536696850337469
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
